You Only Need Less Attention at Each Stage in Vision Transformers
Shuoxi Zhang1
, Hanpeng Liu1*, Stephen Lin2
, Kun He1â€ 
{zhangshuoxi, hanpengliu, brooklet60}@hust.edu.cn, stevelin@microsoft.com
1Huazhong University of Science and Technology, 2Microsoft Research Asia
Abstract
The advent of Vision Transformers (ViTs) marks a subï¿¾stantial paradigm shift in the realm of computer vision.
ViTs capture the global information of images through selfï¿¾attention modules, which perform dot product computations
among patchified image tokens. While self-attention modï¿¾ules empower ViTs to capture long-range dependencies,
the computational complexity grows quadratically with the
number of tokens, which is a major hindrance to the practiï¿¾cal application of ViTs. Moreover, the self-attention mechaï¿¾nism in deep ViTs is also susceptible to the attention satï¿¾uration issue. Accordingly, we argue against the necesï¿¾sity of computing the attention scores in every layer, and
we propose the Less-Attention Vision Transformer (LaViT),
which computes only a few attention operations at each
stage and calculates the subsequent feature alignments in
other layers via attention transformations that leverage the
previously calculated attention scores. This novel approach
can mitigate two primary issues plaguing traditional selfï¿¾attention modules: the heavy computational burden and atï¿¾tention saturation. Our proposed architecture offers supeï¿¾rior efficiency and ease of implementation, merely requiring
matrix multiplications that are highly optimized in contemï¿¾porary deep learning frameworks. Moreover, our architecï¿¾ture demonstrates exceptional performance across various
vision tasks including classification, detection and segmenï¿¾tation.
1. Introduction
In recent years, computer vision has experienced rapid
growth and development, primarily owing to the advances
in deep learning and the accessibility of large datasets.
Among the prominent deep learning techniques, Convoï¿¾lutional Neural Networks (CNNs) [8] have proven parï¿¾ticularly effective, demonstrating exceptional performance
across a wide range of applications, including image classi-
*Equal contribution
â€ Corresponding author
fication [8, 28], object detection [5, 22], and semantic segï¿¾mentation [1, 23].
Inspired by the great success of Transformers [27] in natï¿¾ural language processing, Vision Transformers (ViTs) [4]
divide each image into a set of tokens. These tokens are
then encoded to produce an attention matrix that serves
as a fundamental component of the self-attention mechaï¿¾nism. The computational complexity of the self-attention
mechanism grows quadratically with the number of tokens,
and the computational burden becomes heavier with higherï¿¾resolution images. Some researchers attempt to reduce toï¿¾ken redundancy through dynamic selection [9, 21] or toï¿¾ken pruning [33] to alleviate the computational burden of
the attention computation. These approaches have demonï¿¾strated comparable performance to the standard ViT. Howï¿¾ever, methods involving token reduction and pruning necesï¿¾sitate meticulous design of the token selection module and
may result in the inadvertent loss of critical tokens. In this
work, we explore a different direction and rethink the mechï¿¾anism of self-attention. In the attention saturation problem
raised in [38], as the layers of ViTs are progressively deepï¿¾ened, the attention matrix tends to remain largely unaltered,
mirroring the weight allocation observed in the preceding
layers. Taking these considerations into account, we are
prompted to pose the following question:
Is it really necessary to consistently apply the selfï¿¾attention mechanism throughout each stage of the network,
from inception to conclusion?
In this paper, we propose to modify the fundamenï¿¾tal architecture of standard ViT by introducing the Lessï¿¾Attention Vision Transformer (LaViT). Our framework, as
depicted in Fig. 1, consists of Vanilla Attention (VA) layï¿¾ers and our proposed Less Attention (LA) layers to capï¿¾ture the long-range relationships. In each stage, we excluï¿¾sively compute the traditional self-attention and store the
attention scores in a few initial Vanilla Attention (VA) layï¿¾ers. In subsequent layers, we efficiently generate attention
scores by utilizing the previously calculated attention maï¿¾trices, thereby mitigating the quadratic computational exï¿¾pense associated with self-attention mechanisms. Moreï¿¾over, we integrate residual connections within the attention
arXiv:2406.00427v1 [cs.CV] 1 Jun 2024
Stage 1 Stage 2 Stage 3 Stage 4
H Ã— ğ‘Š Ã— 3
ğ¹1:
ğ»
4
Ã—
ğ‘Š
4
Ã— ğ‘ªğŸ ğ¹2:
ğ»
8
Ã—
ğ‘Š
8
Ã— ğ‘ªğŸ ğ¹3:
ğ»
16
Ã—
ğ‘Š
16
Ã— ğ‘ªğŸ‘ ğ¹4:
ğ»
32
Ã—
ğ‘Š
32
Ã— ğ‘ªğŸ’
Stage i Position
Embedding
Element-wise
Add
Feature
Map
Attention
Matrix Vanilla Attention Layer (ğ¿ğ‘š
ğ‘‰ğ´ Ã—) Less Attention Layer (ğ¿ğ‘š âˆ’ ğ¿ğ‘š
ğ‘‰ğ´
) Ã—)
â‹¯
Figure 1. The architecture of our Less-Attention Vision Transformer (LaViT). The bottom part: the proposed Less-Attention layer, which
together with conventional Transformer blocks in the preceding layers constitutes the feature extraction module of this stage.
layers during downsampling across stages, allowing for the
preservation of crucial semantic information learned in earï¿¾lier stages while still transmitting global contextual inforï¿¾mation through alternate pathways. Finally, we carefully
design a novel loss to preserve the diagonality of attenï¿¾tion matrices during the transformation process. These key
components enable our proposed ViT model to diminish
both computational complexity and attention saturation, ulï¿¾timately leading to notable performance improvements with
reduced floating-point operations per second (FLOPs) and
considerable throughput.
To verify the effectiveness of our proposed approach, we
conduct comprehensive experiments on various benchmark
datasets, comparing the performance of our model with
existing state-of-the-art ViT variants (also recent efficient
ViTs). The experimental results demonstrate the efficacy of
our approach in addressing attention saturation and achievï¿¾ing superior performance in visual recognition tasks.
Our main contributions are summarized as follows:
â€¢ We present a novel ViT architecture that generates atï¿¾tention scores by re-parameterizing the attention matrix
computed by preceding layers. This approach addresses
both the attention saturation and the associated computaï¿¾tional burden.
â€¢ Moreover, we propose a novel loss function that endeavï¿¾ors to preserve the diagonality of attention matrices durï¿¾ing the process of attention re-parameterization. We posit
that this is essential to uphold the semantic integrity of
attention, ensuring that the attention matrices accurately
reflect the relative importance among input tokens.
â€¢ Our architecture consistently performs favorably against
several state-of-the-art ViTs, while having similar or even
reduced computational complexity and memory conï¿¾sumption, across various vision tasks including classifiï¿¾cation, detection and segmentation.
2. Related Work
2.1. Vision Transformers
The Transformer architecture, initially introduced for maï¿¾chine translation [27], has since been applied to computer
vision tasks through the growth of ViT [4]. The key innoï¿¾vation of ViT lies in its capability to capture long-range deï¿¾pendencies between distant regions of the image, achieved
through the incorporation of self-attention mechanisms.
Drawn from the triumph of ViT, a plethora of variant
models have emerged, each devised to ameliorate specific
constraints inherent to the original architecture. For inï¿¾stance, DeiT [25] enhances data efficiency during trainï¿¾ing by incorporating the distillation token. Additionally,
CvT [32] and CeiT [36] integrate the convolutional strucï¿¾ture into the ViT framework to combine the strengths of
CNNs (spatial invariance) and ViTs (long-range depenï¿¾dency modeling). These advancements underscore the onï¿¾going evolution of transformer-based architectures in the
field of computer vision.
2.2. Efficient Vision Transformers
Though highly effective, ViTs suffer from a huge comï¿¾putational burden. The research on efficient vision transï¿¾formers addresses the quadratic cost of the self-attention
operation by including hierarchical downsampling operaï¿¾tions [17, 29, 30], token reduction [9, 21, 33], or lightweight
architectural designs [18, 19]. Hierarchical downsamï¿¾pling operations address the quadratic computation of selfï¿¾attention by reducing the token numbers gradually across
the stages [17, 18, 29, 30] and enable ViTs to learn hierarï¿¾chical structures. Another research direction introduces the
token selection module to eliminate the least meaningful toï¿¾kens and reduce the computational burden. For instance,
[9, 21, 33] reorganize image tokens by preserving informa- Patch Emb
Encoder
Patch Emb
Encoder
Patch Emb
Encoder
Patch Emb
Encoder
Patch Emb
Norm
MHSA
Norm
MLP
Linear
Transpose
Linear
Transpose
Softmax
MatMul
Figure 2. The downsampling on attention across stages.
tive image tokens and dropping ones with little attention to
expedite subsequent MHSA and FFN computations.
2.3. Attention Mechanisms
The key component of ViTs is the attention mechanism,
which computes pairwise interactions between all patches,
resulting in quadratic complexity with respect to the input
size. This problem leads to heavy inference computation,
which hinders the practical application of ViTs in the real
world. Several studies argue that the computational burden
can be alleviated by utilizing sparse attention mechanisms,
which selectively attend to a subset of patches based on their
relevance or proximity. One notable approach is the Adapï¿¾tive Sparse Token Pruning framework [31], which induces
a sparse attention matrix, effectively addressing computaï¿¾tional efficiency concerns. Furthermore, employing techï¿¾niques like structured sparsity patterns [3, 6, 21] can furï¿¾ther reduce computational complexity, thereby enhancing
the overall efficiency of ViTs. Another urgent issue to be
addressed is the problem of attention saturation, where the
attention matrix displays limited variation as the layer depth
increases. This issue has been acknowledged in studies such
as DeepViT [38] and CaiT [26], which report that attention
saturation hinders the ability of deep ViTs to capture addiï¿¾tional semantic information and may even reduce training
stability. Therefore, it is essential to carefully design the
self-attention mechanism in ViTs to avoid sub-optimal soï¿¾lutions.
3. Methodology
In this section, we first review the basic design of the hierï¿¾archical vision transformer. Then, we discuss two major
weaknesses of its attention mechanism, and propose that
self-attention can be grasped with less attention at each
stage. We dynamically re-parameterize the attention scores
by utilizing the stored attention matrix from the previous
layer, effectively mitigating the issue of attention saturation.
Additionally, we integrate residual connections to facilitate
the transfer of global relationships from earlier stages. Last
but not least, we introduce a novel loss, the Diagonality Preï¿¾serving loss, to preserve basic properties in the transformed
attention (i.e., representing the relationships among tokens).
3.1. Vision Transformer
Let x âˆˆ R
HÃ—WÃ—C represent an input image, where
H Ã— W denotes the spatial resolution and C the numï¿¾ber of channels. We first tokenize the image by partiï¿¾tioning it into N = HW/p
2 patches, where each patch
Pi âˆˆ R
pÃ—pÃ—C (i âˆˆ {1, . . . , N}) has a size of p Ã— p pixï¿¾els and C channels. The patch size p is a hyper-parameter
that determines the granularity of the token. The patch emï¿¾bedding can be extracted by using a convolution operator
with the stride and kernel size equal to the patch size. Each
patch is then projected to the embedding space Z âˆˆ R
NÃ—D
through non-overlapping convolution, where D represents
the dimension of each patch.
Multi-Head Self-Attention. We first provide a brief
overview on the vanilla self-attention mechanism that proï¿¾cesses the embedded patches and functions within the
framework of Multi-Head Self-Attention blocks (MHSAs).
In the l-th MHSA block, the input Zlâˆ’1, l âˆˆ {1, Â· Â· Â· , L},
is projected into three learnable embeddings {Q, K, V} âˆˆ
R
NÃ—D. The multi-head attention aims to capture the atï¿¾tention from different views; for simplicity, we choose H
heads, where each head is a matrix with the dimension
N Ã—
D
H
. The h-th head attention matrix Ah can be calï¿¾culated by:
 \mathbf {A}_h = \mathrm {Softmax} \left (\frac {\mathbf {Q}_h \mathbf {K}_h^\mathsf {T}}{\sqrt {d}} \right ) \in \mathbb {R}^{N \times N}. \label {eq:attn} (1)
Ah, Qh, and Kh are the attention matrix, query, and key
of the h-th head, respectively. We also split the value V
into H heads. To avoid vanishing gradients caused by the
sharpness of the probability distribution, we divide the inner
product of Qh and Kh by âˆš
d (d = D/H). The attention
matrix is concatenated as:
 \begin {split} \mathbf {A} &= \textrm {Concat}(\mathbf {A}_1, \cdots , \mathbf {A}_h, \cdots ,\mathbf {A}_H); \\ \mathbf {V} &= \textrm {Concat}(\mathbf {V}_1, \cdots , \mathbf {V}_h, \cdots ,\mathbf {V}_H). \end {split} \label {eq:concat} 
(2)
The calculated attention among spatially split tokens may
guide the model to focus on the most valuable tokens within
the visual data. Subsequently, weighted linear aggregation
is applied to the corresponding value V:
 \boldsymbol {Z}^{\textrm {MHSA}} = \mathbf {AV} \in \mathbb {R}^{N \times D}. \label {eq:val-feats} (3)
Downsampling Operation. Several studies have inï¿¾corporated hierarchical structures into ViTs, drawing inï¿¾spiration from the success of hierarchical architectures in
CNNs. These works partition the Transformer blocks into
M stages and apply downsampling operations before each
Transformer stage, thereby reducing the sequence length.
In our study, we employ a downsampling operation using
a convolutional layer with the kernel size and stride set to
2. This approach permits the flexible adjustment of the feaï¿¾ture mapâ€™s scale at each stage, thereby establishing a Transï¿¾former hierarchical structure that mirrors the organization
of the human visual system.
3.2. The Less-Attention Framework
The overall framework of our network architecture is illusï¿¾trated in Fig. 1. In each stage, we extract the feature repï¿¾resentation in two phases. At the initial several Vanilla Atï¿¾DWConv
Conv 
Layer 
Scale
tention (VA) layers, we conduct the standard MHSA operaï¿¾tion to capture the overall long-range dependencies. Subï¿¾sequently, we simulate the attention matrices to mitigate
quadratic computation and address attention saturation at
the following Less-Attention (LA) layers by applying a linï¿¾ear transformation to the stored attention scores. Herein,
we denote the attention score before the Softmax function
of the initial l-th VA layer in the m-th stage as AVA,l
m , which
is computed by the following standard procedure:
 \mathbf {A}^{\text {VA},l}_m = \frac {\mathbf {Q}^l_m(\mathbf {K}^l_m)^\mathsf {T}}{\sqrt {d}}, ~~ l \leq L^{\text {VA}}_m. \label {eq:init} (4)
Here, Ql
m and Kl
m represent the queries and keys from
the l-th layer of the m-th stage, following the downsamï¿¾pling from the preceding stage. And L
VA
m is used to denote
the number of VA layers. After the initial vanilla attention
phase, we discard the traditional quadratic MHSA and apï¿¾ply transformations on AVA
m to lessen the amount of attenï¿¾tion computation. This process entails two linear transforï¿¾mations with a matrix transposition operation in between.
To illustrate, let us consider the attention matrix in the l-th
(l > LVA
m ) layer (LA layer) of the stage:
 \begin {aligned} &\mathbf {A}^{l}_m = \Psi (\Theta (\mathbf {A}^{l-1}_m)^\mathsf {T})^\mathsf {T}, ~~ L^{\text {VA}}_m<l \leq L_m,\\ &\mathbf {Z}^{\text {LA},l} = \textrm {Softmax}(\mathbf {A}^l_m)\mathbf {V}^l. \end {aligned} 
(5)
In this context, the transformations denoted by Î¨ and Î˜
refer to linear transformation layers with a dimension of
R
NÃ—N . Here, Lm, L
VA
m represent the number of layers and
the number of VA layers in the m-th stage, respectively.
The insertion of the transposition operation between these
two linear layers serves the purpose of maintaining the maï¿¾trix similarity behavior. This step is essential due to the fact
that the linear transformation in a single layer conducts the
transformations row-wise, which could potentially result in
the loss of diagonal characteristics.
3.3. Residual-based Attention Downsampling
When the computation traverses across stages in hierarchiï¿¾cal ViTs, downsampling operations are often employed on
the feature map. While this technique reduces the token
numbers, it may result in the loss of essential contextual inï¿¾formation. Consequently, we posit that the attention affinity
learned from the preceding stage could prove advantageous
for the current stage in capturing more intricate global reï¿¾lationships. Drawing inspiration from ResNet [7], which
introduces shortcut connections to mitigate feature saturaï¿¾tion issues, we adopt a similar concept and incorporate it
into the downsampling attention computation within our arï¿¾chitecture. By introducing a shortcut connection, we can
introduce the inherent bias into the current MHSA block.
This allows the attention matrix from the previous stage to
effectively guide the attention computation of the current
stage, thereby preserving crucial contextual information.
However, directly applying the shortcut connection to
the attention matrix might pose challenges in this context,
primarily due to the difference in the attention dimensions
between the current stage and the preceding stage. Here,
we design an Attention Residual (AR) module that conï¿¾sists of a depth-wise convolution (DWConv) and a Conv1Ã—1
layer to downsample the attention map from the previous
stage while keeping the semantic information. We denote
the last attention matrix (at the Lmâˆ’1 layer) of the previï¿¾ous stage (the m âˆ’ 1-th stage) as A
last
mâˆ’1
, and the downsamï¿¾pled initial attention matrix of the current (the m-th) stage
as A
init
m . A
last
mâˆ’1 has the dimension of R
BÃ—HÃ—Nmâˆ’1Ã—Nmâˆ’1
(Nmâˆ’1 denotes the token number at the m âˆ’ 1-th stage).
We view the multi-head dimension H as the channel dimenï¿¾sion in regular image space, thus with the DWConv operator
(stride = 2, kernel size = 2), we may capture the spatial
dependencies among the tokens during attention downsamï¿¾pling. The output matrix after the DWConv transformation
fits the size of the attention matrix of the current stage, i.e.,
R
BÃ—HÃ—NmÃ—Nm(Nm = Nmâˆ’1/2 in our case). After depthï¿¾width convolution on the attention matrix, we then perform
Conv1Ã—1 to exchange information across different heads.
Our attention downsampling is illustrated in Fig. 2, and the
transformation from A
last
mâˆ’1
to A
init
m can be expressed as:
 \textbf {A}^\textrm {init}_m &= \textrm {Conv}_{1\times 1}\left (\textrm {Norm}(\textrm {DWConv}(\textbf {A}^\textrm {last}_{m-1}))\right ), \label {eq:residual} \\ \mathbf {A}^{\text {VA}}_m &\gets \mathbf {A}^{\text {VA}}_m + \textrm {LS}(\textbf {A}^\textrm {init}_m) \label {eq:plus},
(7)
where LS is the Layer-Scale operator introduced in [26] to
alleviate attention saturation. AVA
m is the attention score
for the first layer in the m-th stage, which is calculated by
adding the standard MHSA with Eq. 4 and the residual calï¿¾culated by Eq. 6.
Two fundamental design principles guide our attenï¿¾tion downsampling module. First, we utilize DWConv to
capture spatial local relationships during downsampling,
thereby enabling the efficient compression of attention reï¿¾lationships. Second, the Conv1Ã—1 operation is utilized to
exchange the attention information across heads. This deï¿¾sign is pivotal as it facilitates the efficient propagation of
attention from the preceding stage to the subsequent stage.
Incorporating the residual attention mechanism necessitates
only minor adjustments, typically involving adding a few
lines of code to the existing ViT backbone. It is worth emï¿¾phasizing that such a technique can be seamlessly applied to
various versions of the Transformer architecture. The only
prerequisite is to store the attention scores from the previï¿¾ous layer and establish the skip-connections to this layer
accordingly. The importance of this module will be further
illuminated through comprehensive ablation studies.
3.4. Diagonality Preserving Loss
We have carefully designed the Transformer modules by
incorporating attention transformation operators, aiming to
mitigate the issues of computational cost and attention satï¿¾uration. However, a pressing challenge remainsâ€”ensuring
that the transformed attention preserves the inter-token reï¿¾lationships. It is well established that applying transformaï¿¾tions to attention matrices can compromise their capacity to
capture similarities, largely due to the linear transformation
treating the attention matrix row-wise. Thus, we design a
an alternative approach to guarantee that the transformed atï¿¾tention matrix retains the fundamental properties necessary
to convey associations among tokens. A conventional atï¿¾tention matrix should possess the following two properties,
i.e., diagonality and symmetry:
 \begin {aligned}[b] \mathbf {A}_{ij} &= \mathbf {A}_{ji}, \\ \mathbf {A}_{ii} &> \mathbf {A}_{ij}, \forall j \neq i. \end {aligned} \label {eq:property} 
(8)
Thus, we design the diagonality preserving loss of the l-th
layer to keep these two basic properties as:
 \begin {split} {\mathcal {L}_{\textrm {DP},l}} &= \sum _{i=1}^N\sum _{j=1}^N\left |\mathbf {A}_{ij} -\mathbf {A}_{ji}\right | \\ &+ \sum _{i=1}^N((N-1)\mathbf {A}_{ii}-\sum _{j\neq i}\mathbf {A}_{j}). \end {split} 
(9)
Here, LDP is the Diagonality Preserving loss aiming at preï¿¾serving the properties of attention matrix of Eq. 8. We add
our Diagonality Preserving Loss on all transformation layï¿¾ers with the vanilla cross-entropy (CE) loss [4], thus the
total loss in our training can be presented as:
 \begin {aligned}[b] \mathcal {L}_\textrm {total} &= \mathcal {L}_\textrm {CE} + \sum _{m=1}^M\sum _{l=1}^{L_m}\mathcal {L}_{\textrm {DP},l}, \\ \mathcal {L}_\textrm {CE} &= \textrm {cross-entropy}(Z_\texttt {Cls}, y), \end {aligned} 
(10)
where ZCls is the classification token of the representation
in the last layer.
3.5. Complexity Analysis
Our architecture consists of four stages, each comprising
Lm layers. The downsampling layer is applied between
each consecutive stage. As such, the computational comï¿¾plexity of traditional self-attention is O(N2
mD), whereas
the associated K-Q-V transformation incurs a complexï¿¾ity of O(3NmD2
). In contrast, our method leverages an
Nm Ã— Nm linear transformation within the transformation
layer, thereby circumventing the need for computing the inï¿¾ner products. Consequently, the computation complexity
of our attention mechanism in the transformation layer is
Table 1. Detailed configurations of the LaViT series. â€™Blocksâ€™ and
â€™Headsâ€™ refer to the number of blocks ([L
1
, L2
, L3
, L4
]) and heads
in four stages, respectively. â€™Channelsâ€™ refers to the input channel
dimensions across the four stages. And â€™NLAâ€™ denotes the layer
within each stage at which the utilization of the Less-Attention
layer begins.
Models Channels Blocks Heads NLA
LaViT-T [64,128,320,512] [2,2,2,2] [1,2,5,8] [0,0,2,2]
LaViT-S [64,128,320,512] [3,4,6,3] [1,2,5,8] [0,0,3,2]
LaViT-B [64,128,320,512] [3,3,18,3] [1,2,5,8] [0,2,4,3]
reduced to O(N2
m), representing a reduction factor of D.
Additionally, since our method calculates the query emï¿¾beddings solely within the Less-Attention layer, our K-Q-V
transformation complexity is likewise diminished by a facï¿¾tor of 3.
In the downsampling layer between consecutive stages,
considering a downsample rate of 2 as an example, the
computational complexity of the DWConv in the attention
downsampling layer can be calculated as Complexity =
2Ã—2Ã—
Nm
2 Ã—
Nm
2 Ã—D = O(N2
mD). Similarly, the complexï¿¾ity of the Conv1Ã—1 operation in the attention residual modï¿¾ule is also O(N2
mD). However, it is important to note that
attention downsampling only occurs once per stage. Thereï¿¾fore, the additional complexity introduced by these operaï¿¾tions is negligible when compared to the complexity reducï¿¾tion achieved by the Less-Attention layer.
4. Experiments
In this section, we evaluate our modelâ€™s performance on two
benchmark datasets: ImageNet-1K [8] for classification,
COCO2017 [10] for detection and ADE20K [37] for segï¿¾mentation. We compare our model with other state-of-theï¿¾art works on these datasets to demonstrate its effectiveness
and efficiency. Furthermore, we perform ablation studies to
investigate the necessity and contributions of each compoï¿¾nent in the proposed model. This analysis provides valuable
insights into the role of each part and helps to establish the
efficacy of our approach.
4.1. Architecture Variants
To ensure an equitable comparison with other models while
maintaining a similar level of computational complexity,
we establish three models: LaViT-T, LaViT-S, and LaViTï¿¾B. The detailed configuration information is provided in
Table 1, and we follow the same network structure as
1The 90% notation in brackets indicates that we keep the token ratio of
90% to represent the visual data during the training of the corresponding
ViTsâ€”DynamicViT and EviT, respectively. Additionally, given our aim
to strike a balance between efficiency and effectiveness, we will not comï¿¾pare our results to high-performance but computationally intensive models,
such as Swin-B-V2 [14] and ConvNeXt-B [15].
Model Params FLOPs Throughput Top1
(M) (G) (image/s) (%)
ResNet-18 11.7 1.8 4454 69.8
RegNetY-1.6G 11.2 1.6 1845 78.0
DeiT-T 5.7 1.3 3398 72.2
PVT-T 13.2 1.9 1768 75.1
PVTv2-b1 13.1 2.1 1231 78.7
LaViT-T 10.9 1.6 2098 79.2
ResNet-50 25.0 4.1 1279 76.2
RegNetY-4G 20.6 4.0 1045 79.4
EfficientNet-B4 19.0 4.2 387 82.4
EfficientViT-B2 24.0 4.5 1587 82.1
DeiT-S 22.1 4.6 1551 79.9
DeepViT-S 27.0 6.2 1423 82.3
PVT-S 24.5 3.8 1007 79.8
CvT-S 25.8 7.1 636 82.0
Swin-T 28.3 4.5 961 81.2
PVTv2-b2 25.4 4.0 695 82.0
DynamicViT-S (90%) 24.1 4.0 1524 79.8
EViT-S (90%) 23.9 4.1 1706 79.7
LiT-S 27.0 4.1 1298 81.5
PPT-S 22.1 3.1 1698 79.8
LaViT-S 22.4 3.3 1546 82.6
ResNet-101 45.0 7.9 722 77.4
ViT-B 86.6 17.6 270 77.9
DeiT-B 86.6 17.5 582 81.8
Swin-S 49.6 8.7 582 83.1
Swin-B 87.8 15.4 386 83.4
DynamicViT-B (90%) 76.6 14.1 732 81.5
EViT-B (90%) 78.6 15.3 852 81.3
LiT-M 48.0 8.6 638 83.0
PPT-B 86.0 14.5 714 81.4
PVT-M 44.2 6.7 680 81.2
PVT-L 61.4 9.8 481 81.7
LaViT-B 39.6 6.1 877 83.1
Table 2. Comparison of different backbones on ImageNet-1K clasï¿¾sification. Except for EfficientNet (EfficientNet-B4), all models
are trained and evaluated with an input size of 224 Ã— 224. The
least computations and fastest throughput appear in blue bold, and
the best results appear in bold.
1
PVT [29, 30] except for introducing the Less-Attention
Transformer encoder and skip-connection attention downï¿¾sampling. The number of blocks, channels, and heads afï¿¾fects the computational cost.
4.2. Baselines
We conduct a thorough experimental evaluation of our proï¿¾posed method by comparing it with various CNNs, ViTs,
and hierarchical ViTs. Specifically, the following baselines
are used:
â€¢ CNNs: ResNet [7], RegNet [20] and EfficientNet [24].
â€¢ ViTs: ViT [4], DeiT [25], CvT [36],DeepViT [38], Foï¿¾calViT [35] and SwinTransformer [13].
â€¢ Efficient ViTs: HVT [17], PVT [29], DynamicViT [21],
EViT [9], LiT [19], EfficientViT [2] and PPT [33].
4.3. Image Classification on ImageNet-1K
Settings. The image classification experiments are conï¿¾ducted on the ImageNet-1K dataset. Our experimental proï¿¾tocol follows the procedures outlined in DeiT [25], with the
exception of the model itself. Specifically, we apply the
same data augmentation and regularization techniques emï¿¾ployed in DeiT. We utilize the AdamW optimizer [16] to
train our models from scratch for 300 epochs (with a 5-
epoch warm-up). The initial learning rate is set to 0.005 and
varies according to a cosine scheduler. The global batch size
is set to 1024, distributed across 4 GTX-3090 GPUs. Durï¿¾ing the test on the validation set, the input images are first
resized to 256 pixels, followed by a center crop of 224 x
224 pixels to evaluate the classification accuracy.
Results. We present the classification results on ImageNet-
1K in Table 2. The models are classified into three groups
based on their computational complexity: tiny (approxiï¿¾mately 2G), small (approximately 4G), and base (approxï¿¾imately 9G). Our approach achieves competitive perforï¿¾mance compared to state-of-the-art ViTs with markedly reï¿¾duced computational requirements. Specifically, in the tiny
and small model clusters, our method surpasses all other exï¿¾isting models by at least 0.2% and 0.5%, respectively, while
maintaining a substantially lower computational cost, which
is our principal concern. In the base-size models, our archiï¿¾tecture, which incorporates the base structure of PVT but
includes the Less-Attention component, demonstrates supeï¿¾rior performance over two PVT-based models (PVT-M and
PVT-L). Furthermore, we also compare our architecture to
several efficient ViT designs (DynamicViT, EViT, LiT, effiï¿¾cientViT and PPT). We observe that our results reflect a betï¿¾ter balance between effectiveness and efficiency. Note that
our design necessitates reduced computation cost owing to
our resource-efficient Less-Attention mechanism, rendering
our lightweight module an attractive option for implementï¿¾ing ViT on mobile platforms.
4.4. Object Detection on COCO2017
Settings. We conduct the detection experiments on COCO
2017 [10] dataset. We test the model effectiveness on Retiï¿¾naNet [12]. We follow the common practice by initializï¿¾ing the backbone with pre-trained weights obtained from
ImageNet-1K. In addition, we use AdamW [16] optimizer,
and train the network with the batchsize of 16 on 8 GPUs.
Results. We present the results of object detection in Taï¿¾ble 3. It is evident that our LaViT model exhibits a noï¿¾table advantage over both its CNN and Transformer counï¿¾terparts. Specifically, with the 1Ã— schedule, our tiny verï¿¾sion LaViT-T achieves 9.9-12.5 APb
against ResNet under
comparable settings, while the small version LaViT-S outï¿¾performs its CNN counterpart by 8.1-10.3 APb
. This trend
persists with the 3Ã— schedule, as our LaViT consistently
demonstrates competitive performance. Particularly noteï¿¾worthy is our architectureâ€™s ability to consistently outperï¿¾form the Swin Transformer in terms of detection perforï¿¾mance while imposing a smaller training burden. Thus,
the results on COCO2017 reaffirm our assertion that our
Backbone #Param. FLOPs RetinaNet 1Ã— RetinaNet 3Ã— + MS
(M) (G) APb APb
50 APb
75 APb
S APbM APb
L APb APb
50 APb
75 APb
S APbM APb
L
ResNet50 38 239 36.3 55.3 38.6 19.3 40.0 48.8 39.0 58.4 41.8 22.4 42.8 51.6
PVT-Small 34 226 40.4 61.3 43.0 25.0 42.9 55.7 42.2 62.7 45.0 26.2 45.2 57.2
Swin-T 39 245 41.5 62.1 44.2 25.1 44.9 55.5 43.9 64.8 47.1 28.4 47.2 57.8
LaViT-T(ours) 33 202 46.2 67.2 49.1 29.6 50.2 61.3 48.4 69.9 51.7 31.8 52.2 64.1
ResNet101 58 315 38.5 57.8 41.2 21.4 42.6 51.1 40.9 60.1 44.0 23.7 45.0 53.8
PVT-M 54 283 41.9 63.1 44.3 25.0 44.9 57.6 43.2 63.8 46.1 27.3 46.3 58.9
Swin-S 60 335 44.5 65.7 47.5 27.4 48.0 59.9 46.3 67.4 49.8 31.1 50.3 60.9
LaViT-S(ours) 47 290 46.7 68.3 49.7 29.9 50.7 61.7 48.9 70.3 52.2 33.1 52.6 65.4
Table 3. Results on COCO object detection using the RetinaNet [12] framework. 1Ã— refers to 12 epochs, and 3Ã— refers to 36 epochs. MS
means multi-scale training. APb
and APm denotes box mAP and mask mAP, respectively. FLOPs are measured at resolution 800 Ã— 1280.
5 10 15 20 25
Layer Index
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
ViT
ViT+LA
(a) ViT
5 10 15
Layer Index
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
PVT
PVT+LA
(b) PVT
Figure 3. The similarity ratio of the generated self-attention maps
of the current layer with its previous layer.
carefully designed LaViT model enhances feature extracï¿¾tion with reduced computational overhead.
4.5. Semantic Segmentation on ADE20K
Settings. We conduct experiments on semantic segmenï¿¾tation using the ADE20K dataset, which comprises 150
classes and 20,000 images for training, and 2,000 images
for validation. Our backbone networks for segmentation
are Semantic FPN [11] and UperNet [34]. We follow the
training settings established in [13] and resize images to
512 Ã— 512 for training. We train UperNet for 160k iteraï¿¾tions and SemanticFPN for 80k iterations. The initial learnï¿¾ing rate is set to 6 Ã— 10âˆ’5
, utilizing a poly scheduler for
learning rate decay. The experiment is conducted by using
the batch size of 16 across 4 GTX3090 GPUs.
Results. Table 4 provides an overview of the segmentaï¿¾tion results. Our model demonstrates superiority over Swin
Transformer, exhibiting an mIoU improvement of +2.6 with
Semantic FPN and +2.7 with UperNet. In the Semantic FPN
test, our LaViT-S achieves a relatively modest increase of
+0.9 mIoU compared to the baseline (PVT-S), but notably
with significantly fewer computations. When integrated
into the UperNet architecture, LaViT achieves substantial
improvements of +2.7 mIoU, +1.0 mIoU, and +1.4 mIoU
when compared to various mainstream models. These comï¿¾petitive results are maintained even when employing test
time augmentation. In particular, LaViT-S outperforms
Focal-T by +1.4 mIoU and +2.5 MS mIOU. These findings
underscore LaViTâ€™s ability to produce high-quality semanï¿¾tic segmentation outputs while operating within the frameï¿¾work of its computation-efficient attention mechanism.
4.6. Ablation Study
Attention Saturation. To demonstrate the efficacy of our
Less-Attention module in addressing attention saturation,
we present the attention similarity ratio (cosine similarity
computed by the attention map in the current layer and its
previous layer) in Figure 3. We conduct the comparison
using two backbones, namely, ViT and PVT. In 3a, we seï¿¾lect the ViT architecture with 25 layers and no hierarchical
structure. In 3b, we employ PVT-M as our baseline and
assess the attention similarity at the 3rd stage, which conï¿¾sists of 18 layers. Both sets of results clearly illustrate that
the original architecture encounters a significant attention
saturation issue. However, this phenomenon is effectively
mitigated by incorporating our modules, enabling deep
attention to fulfill its intended role.
Extendability of Less-Attention Module. We extend our
Less-Attention module to various ViT architectures, and
report the results in Table 5. The incorporation of the Lessï¿¾Attention layer into any of the foundational Transformer
architectures leads to enhancements in accuracy while
concurrently reducing computational demands. Notably,
the most significant improvement is observed when incorï¿¾porating the module into the vanilla ViT/DeiT architecture.
This may be attributed to the fact that the vanilla ViT/DeiT
does not have a hierarchical structure, thereby experiencing
considerable attention saturation issues. Moreover, when
integrating our method into DeepViT, we observe the most
substantial decrease in computational resources. These
findings jointly underscore the scalability of our method,
demonstrating that the application of LA module can render
existing ViT architectures more practical and feasible.
Importance of Each Component. We conduct ablation
studies on the proposed module with the ImageNet-1k
dataset, and the results are shown in Table 6. On both
networks (i.e., tiny and small), our proposed modules prove
to be indispensible for Transformer training. The baseline,
which replaces the Less-Attention layer with MHSA, corï¿¾responds exactly to the PVT model, exhibiting a decrease
in predictive accuracy by 0.5% and 0.6% compared to
our model. Additionally, removing the attention residual
modules, denoted as â€œw/o ARâ€, results in a reduction of
predictive accuracy by 0.2% and 0.4%. Lastly, and most
Cosine Similarity
Backbone Semantic FPN 80k UperNet 160K
Param (M) FLOPs (G) mIOU (%) Param (M) FLOPs (G) mIOU (%) MS mIOU (%)
ResNet-50 28.5 183 36.7 - - - -
Swin-T 31.9 182 41.5 59.9 945 44.5 45.8
PVT-S 30.2 146 43.2 - - - -
Twin-S 28.3 144 43.2 54.4 932 46.2 47.1
LiT-S 32.0 172 41.3 57.8 978 44.6 45.9
Focal-T - - - 62.0 998 45.8 47.0
LaViT-S 25.1 122 44.1 52.0 920 47.2 49.5
Table 4. Segmentation performance of different backbones in Semantic FPN and UpperNet framework on ADE20K. The least computation
appears in blue bold, and the best results appear in bold.
Backbone Tiny Small
Top-1 Acc(%) FLOPs (G) Top-1 Acc(%) FLOPs (G)
ViT 72.2 1.4 79.1 4.6
ViT+LA 73.2(â†‘ 1.0) 1.2(â†“ 14.2%) 80.0(â†‘ 0.9) 4.0(â†“ 13.1%)
DeiT 72.2 1.4 79.9 4.7
DeiT+LA 73.4(â†‘ 1.2) 1.2(â†“ 14.2%) 80.4(â†‘ 0.5) 4.2(â†“ 10.6%)
DeepViT 73.4 1.5 80.9 4.8
DeepViT+LA 73.8(â†‘ 0.4) 1.1(â†“ 25.8%) 81.4(â†‘ 0.5) 4.2(â†“ 12.6%)
CeiT 76.2 1.2 82.0 4.5
CeiT+LA 76.7(â†‘ 0.5) 1.1(â†“ 9.0%) 82.4(â†‘ 0.4) 4.1(â†“ 8.8%)
HVT 75.7 1.4 80.4 4.6
HVT+LA 76.2(â†‘ 0.5) 1.2(â†“ 15.2%) 80.8(â†‘ 0.4) 4.2(â†“ 13.4%)
PVT 75.1 1.9 79.8 3.8
PVT+LA 75.9(â†‘ 0.8) 1.4(â†“ 25.6%) 80.4(â†‘ 0.6) 3.2(â†“ 15.7%)
Swin 81.2 4.5 83.2 8.7
Swin+LA 81.7(â†‘ 0.5) 4.0(â†“ 11.1%) 83.5(â†‘ 0.3) 7.8(â†“ 10.3%)
Table 5. Top-1 classification accuracy on ImageNet-1k using
different transformer backbones and their corresponding Lessï¿¾Attention plug-in variants. Footnote â€™LAâ€™ indicates the addition
of our Less-Attention module to the respective backbone archiï¿¾tectures. â†‘ and â†“ denote the increase in Top-1 accuracy and the
percentage of FLOPs reduction, respectively.
Model Module Tiny Small AR LA LDP
w/o LA - - - 78.7 82.0
w/o AR - âœ“ âœ“ 79.0 82.2
LaViT âœ“ âœ“ âœ“ 79.2 82.6
w/o LDP âœ“ âœ“ - 59.1(â†“ 20.1) 57.1(â†“ 25.5)
Table 6. Ablation study of the proposed module on the ImageNet-
1k dataset. Baseline means we remove all the proposed modules,
resulting in the PVT Transformer baseline. â€™ARâ€™ and â€™LAâ€™ indicate
the Attention-Residual and Less-Attention modules, respectively.
â€™w/o LDPâ€™ indicates we remove the Diagonality Preserving loss.
importantly, we assert that the additional loss function
to preserve diagonal correlations is vital for effectively
comprehending semantic information in visual data. When
relying solely on the CE loss, the modelâ€™s predictions
deteriorate. This might be attributed to the potential limiï¿¾tation of relying solely on the transformation for attention
matrices, which could compromise their capacity to express
correlations among tokens. All these experimental findings
collectively emphasize the contribution of each component
within our model architecture.
Model Stage 3 Stage 4
L2 L3 L4 L1 L2 L3
LaViT-S 79.1 82.6 82.4 80.1 82.6 82.3
LaViT-B 78.9 82.5 83.1 80.4 82.3 83.1
Table 7. Ablation study on the layer where Less-Attention starts.
We conduct experiments on the last two stagesâ€”Stage 3,4. â€™L2â€™
beneath Stage 3 means we use the Less-Attention layer to replace
the vanilla encoder from the second layer in the third Stage.
Less-Attention Selection. In deep ViTs, careful selection
of the starting layer for Less-Attention is crucial. Thus,
we design experiments to select the starting layer for Lessï¿¾Attention in the network architecture, and the results are
presented in Table 7. As shown in the table, directly usï¿¾ing the Less-Attention layer from the second layer in the
stage leads to a decrease in model performance. This pheï¿¾nomenon could be attributed to overly relying on the seï¿¾mantics of the first MHSA layer. Thus, leveraging the Lessï¿¾Attention layer at deeper layers in the stage may mitigate
this issue. Furthermore, while utilizing the Less-Attention
layer at relatively deeper layers does not affect the model
performance much, it may lead to increased computational
costs. This contradicts the design objective of our architecï¿¾ture to reduce the computational overhead.
5. Conclusion
Aiming to reduce the costly self-attention computations, we
proposed a new model called Less-Attention Vision Transï¿¾former (LaViT). LaViT leverages the computed dependency
in Multi-Head Self-Attention (MHSA) blocks and bypasses
the attention computation by re-using attentions from previï¿¾ous MSA blocks. We additionally incorporated a straightï¿¾forward Diagonality Preserving loss, designed to promote
the intended behavior of the attention matrix in representï¿¾ing relationships among tokens. Notably, our Transformer
architecture effectively captures cross-token associations,
surpassing the performance of the baseline while maintainï¿¾ing a computationally efficient profile in terms of quanï¿¾tity of parameters and floating-point operations per second
(FLOPs). Comprehensive experimentation has confirmed
the efficacy of our model as a foundational architecture
for multiple downstream tasks. Specifically, the proposed
model demonstrates superiority over previous Transformer
architectures, resulting in state-of-the-art performance in
classification and segmentation tasks.
References
[1] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.
Segnet: A deep convolutional encoder-decoder architecture
for image segmentation. TPAMI, pages 2481â€“2495, 2017. 1
[2] Han Cai, Chuang Gan, and Song Han. Efficientvit: Enhanced
linear attention for high-resolution low-computation visual
recognition. In IEEE/CVF ICCV, 2023. 6
[3] Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang,
and Zhangyang Wang. Chasing sparsity in vision transformï¿¾ers: An end-to-end exploration. NeurIPS, pages 19974â€“
19988, 2021. 3
[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylï¿¾vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In ICLR, 2021. 1, 2, 5, 6
[5] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Region-based convolutional networks for accurate
object detection and segmentation. TPAMI, pages 142â€“158,
2015. 1
[6] Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng, Jiï¿¾aying Liu, and Jingdong Wang. Demystifying local vision
transformer: Sparse connectivity, weight sharing, and dyï¿¾namic weight. arXiv:2106.04263, 2021. 3
[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR,
pages 770â€“778, 2016. 4, 6
[8] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classification with deep convolutional neural netï¿¾works. Commun.ACM, pages 84â€“90, 2017. 1, 5
[9] Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song,
Jue Wang, and Pengtao Xie. Not all patches are what you
need: Expediting vision transformers via token reorganizaï¿¾tions. arXiv:2202.07800, 2022. 1, 2, 6
[10] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr DollÂ´ar, and
C. Lawrence Zitnick. Microsoft COCO: common objects in
context. In ECCV, pages 740â€“755, 2014. 5, 6
[11] Tsung-Yi Lin, Piotr DollÂ´ar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyramid
networks for object detection. In CVPR, pages 2117â€“2125,
2017. 7
[12] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr DollÂ´ar. Focal loss for dense object detection. In Proï¿¾ceedings of the IEEE international conference on computer
vision, pages 2980â€“2988, 2017. 6, 7
[13] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
ICCV, pages 9992â€“10002, 2021. 6, 7
[14] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,
Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.
Swin transformer v2: Scaling up capacity and resolution. In
CVPR, pages 12009â€“12019, 2022. 5
[15] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtï¿¾enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s. In CVPR, pages 11976â€“11986, 2022. 5
[16] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. In ICLR, 2019. 6
[17] Zizheng Pan, Bohan Zhuang, Jing Liu, Haoyu He, and Jianï¿¾fei Cai. Scalable vision transformers with hierarchical poolï¿¾ing. In ICCV, pages 377â€“386, 2021. 2, 6
[18] Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Fast vision
transformers with hilo attention. NeurIPS, pages 14541â€“
14554, 2022. 2
[19] Zizheng Pan, Bohan Zhuang, Haoyu He, Jing Liu, and Jianï¿¾fei Cai. Less is more: Pay less attention in vision transformï¿¾ers. In AAAI, pages 2035â€“2043, 2022. 2, 6
[20] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,
Kaiming He, and Piotr DollÂ´ar. Designing network design
spaces. In CVPR, pages 10428â€“10436, 2020. 6
[21] Yongming rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie
Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision
transformers with dynamic token sparsification. NeurIPS,
34:13937â€“13949, 2021. 1, 2, 3, 6
[22] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Unified, real-time object deï¿¾tection. In CVPR, pages 779â€“788, 2016. 1
[23] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In MICCAI, pages 234â€“241, 2015. 1
[24] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model
scaling for convolutional neural networks. In ICML, pages
6105â€“6114, 2019. 6
[25] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and HervÂ´e JÂ´egou. Training
data-efficient image transformers & distillation through atï¿¾tention. In ICML, pages 10347â€“10357, 2021. 2, 6
[26] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,
Gabriel Synnaeve, and HervÂ´e JÂ´egou. Going deeper with imï¿¾age transformers. In ICCV, pages 32â€“42, 2021. 3, 4
[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoï¿¾reit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. NeurIPS, 30, 2017. 1,
2
[28] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng
Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang.
Residual attention network for image classification. In
CVPR, pages 3156â€“3164, 2017. 1
[29] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao
Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.
Pyramid vision transformer: A versatile backbone for dense
prediction without convolutions. In ICCV, pages 568â€“578,
2021. 2, 6
[30] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao
Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt
v2: Improved baselines with pyramid vision transformer.
Computational Visual Media, pages 415â€“424, 2022. 2, 6
[31] Cong Wei, Brendan Duke, Ruowei Jiang, Parham Aarabi,
Graham W Taylor, and Florian Shkurti. Sparsifiner: Learnï¿¾ing sparse instance-dependent attention for efficient vision
transformers. In CVPR, pages 22680â€“22689, 2023. 3
[32] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,
Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing
convolutions to vision transformers. In ICCV, pages 22â€“31,
2021. 2
[33] Xinjian Wu, Fanhu Zeng, Xiudong Wang, Yunhe Wang, and
Xinghao Chen. Ppt: Token pruning and pooling for efficient
vision transformers. arXiv:2310.01812, 2023. 1, 2, 6
[34] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and
Jian Sun. Unified perceptual parsing for scene understandï¿¾ing. In ECCV, pages 418â€“434, 2018. 7
[35] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang
Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal selfï¿¾attention for local-global interactions in vision transformers.
arXiv:2107.00641, 2021. 6
[36] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengï¿¾wei Yu, and Wei Wu. Incorporating convolution designs into
visual transformers. In ICCV, pages 579â€“588, 2021. 2, 6
[37] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fiï¿¾dler, Adela Barriuso, and Antonio Torralba. Semantic unï¿¾derstanding of scenes through the ADE20K dataset. Int. J.
Comput. Vis., pages 302â€“321, 2019. 5
[38] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang,
Xiaochen Lian, Zihang Jiang, Qibin Hou, and Jiashi
Feng. Deepvit: Towards deeper vision transformer.
arXiv:2103.11886, 2021. 1, 3, 6