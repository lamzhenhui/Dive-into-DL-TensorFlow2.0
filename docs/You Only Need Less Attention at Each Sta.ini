You Only Need Less Attention at Each Stage in Vision Transformers
Shuoxi Zhang1
, Hanpeng Liu1*, Stephen Lin2
, Kun He1†
{zhangshuoxi, hanpengliu, brooklet60}@hust.edu.cn, stevelin@microsoft.com
1Huazhong University of Science and Technology, 2Microsoft Research Asia
Abstract
The advent of Vision Transformers (ViTs) marks a sub￾stantial paradigm shift in the realm of computer vision.
ViTs capture the global information of images through self￾attention modules, which perform dot product computations
among patchified image tokens. While self-attention mod￾ules empower ViTs to capture long-range dependencies,
the computational complexity grows quadratically with the
number of tokens, which is a major hindrance to the practi￾cal application of ViTs. Moreover, the self-attention mecha￾nism in deep ViTs is also susceptible to the attention sat￾uration issue. Accordingly, we argue against the neces￾sity of computing the attention scores in every layer, and
we propose the Less-Attention Vision Transformer (LaViT),
which computes only a few attention operations at each
stage and calculates the subsequent feature alignments in
other layers via attention transformations that leverage the
previously calculated attention scores. This novel approach
can mitigate two primary issues plaguing traditional self￾attention modules: the heavy computational burden and at￾tention saturation. Our proposed architecture offers supe￾rior efficiency and ease of implementation, merely requiring
matrix multiplications that are highly optimized in contem￾porary deep learning frameworks. Moreover, our architec￾ture demonstrates exceptional performance across various
vision tasks including classification, detection and segmen￾tation.
1. Introduction
In recent years, computer vision has experienced rapid
growth and development, primarily owing to the advances
in deep learning and the accessibility of large datasets.
Among the prominent deep learning techniques, Convo￾lutional Neural Networks (CNNs) [8] have proven par￾ticularly effective, demonstrating exceptional performance
across a wide range of applications, including image classi-
*Equal contribution
†Corresponding author
fication [8, 28], object detection [5, 22], and semantic seg￾mentation [1, 23].
Inspired by the great success of Transformers [27] in nat￾ural language processing, Vision Transformers (ViTs) [4]
divide each image into a set of tokens. These tokens are
then encoded to produce an attention matrix that serves
as a fundamental component of the self-attention mecha￾nism. The computational complexity of the self-attention
mechanism grows quadratically with the number of tokens,
and the computational burden becomes heavier with higher￾resolution images. Some researchers attempt to reduce to￾ken redundancy through dynamic selection [9, 21] or to￾ken pruning [33] to alleviate the computational burden of
the attention computation. These approaches have demon￾strated comparable performance to the standard ViT. How￾ever, methods involving token reduction and pruning neces￾sitate meticulous design of the token selection module and
may result in the inadvertent loss of critical tokens. In this
work, we explore a different direction and rethink the mech￾anism of self-attention. In the attention saturation problem
raised in [38], as the layers of ViTs are progressively deep￾ened, the attention matrix tends to remain largely unaltered,
mirroring the weight allocation observed in the preceding
layers. Taking these considerations into account, we are
prompted to pose the following question:
Is it really necessary to consistently apply the self￾attention mechanism throughout each stage of the network,
from inception to conclusion?
In this paper, we propose to modify the fundamen￾tal architecture of standard ViT by introducing the Less￾Attention Vision Transformer (LaViT). Our framework, as
depicted in Fig. 1, consists of Vanilla Attention (VA) lay￾ers and our proposed Less Attention (LA) layers to cap￾ture the long-range relationships. In each stage, we exclu￾sively compute the traditional self-attention and store the
attention scores in a few initial Vanilla Attention (VA) lay￾ers. In subsequent layers, we efficiently generate attention
scores by utilizing the previously calculated attention ma￾trices, thereby mitigating the quadratic computational ex￾pense associated with self-attention mechanisms. More￾over, we integrate residual connections within the attention
arXiv:2406.00427v1 [cs.CV] 1 Jun 2024
Stage 1 Stage 2 Stage 3 Stage 4
H × 𝑊 × 3
𝐹1:
𝐻
4
×
𝑊
4
× 𝑪𝟏 𝐹2:
𝐻
8
×
𝑊
8
× 𝑪𝟐 𝐹3:
𝐻
16
×
𝑊
16
× 𝑪𝟑 𝐹4:
𝐻
32
×
𝑊
32
× 𝑪𝟒
Stage i Position
Embedding
Element-wise
Add
Feature
Map
Attention
Matrix Vanilla Attention Layer (𝐿𝑚
𝑉𝐴 ×) Less Attention Layer (𝐿𝑚 − 𝐿𝑚
𝑉𝐴
) ×)
⋯
Figure 1. The architecture of our Less-Attention Vision Transformer (LaViT). The bottom part: the proposed Less-Attention layer, which
together with conventional Transformer blocks in the preceding layers constitutes the feature extraction module of this stage.
layers during downsampling across stages, allowing for the
preservation of crucial semantic information learned in ear￾lier stages while still transmitting global contextual infor￾mation through alternate pathways. Finally, we carefully
design a novel loss to preserve the diagonality of atten￾tion matrices during the transformation process. These key
components enable our proposed ViT model to diminish
both computational complexity and attention saturation, ul￾timately leading to notable performance improvements with
reduced floating-point operations per second (FLOPs) and
considerable throughput.
To verify the effectiveness of our proposed approach, we
conduct comprehensive experiments on various benchmark
datasets, comparing the performance of our model with
existing state-of-the-art ViT variants (also recent efficient
ViTs). The experimental results demonstrate the efficacy of
our approach in addressing attention saturation and achiev￾ing superior performance in visual recognition tasks.
Our main contributions are summarized as follows:
• We present a novel ViT architecture that generates at￾tention scores by re-parameterizing the attention matrix
computed by preceding layers. This approach addresses
both the attention saturation and the associated computa￾tional burden.
• Moreover, we propose a novel loss function that endeav￾ors to preserve the diagonality of attention matrices dur￾ing the process of attention re-parameterization. We posit
that this is essential to uphold the semantic integrity of
attention, ensuring that the attention matrices accurately
reflect the relative importance among input tokens.
• Our architecture consistently performs favorably against
several state-of-the-art ViTs, while having similar or even
reduced computational complexity and memory con￾sumption, across various vision tasks including classifi￾cation, detection and segmentation.
2. Related Work
2.1. Vision Transformers
The Transformer architecture, initially introduced for ma￾chine translation [27], has since been applied to computer
vision tasks through the growth of ViT [4]. The key inno￾vation of ViT lies in its capability to capture long-range de￾pendencies between distant regions of the image, achieved
through the incorporation of self-attention mechanisms.
Drawn from the triumph of ViT, a plethora of variant
models have emerged, each devised to ameliorate specific
constraints inherent to the original architecture. For in￾stance, DeiT [25] enhances data efficiency during train￾ing by incorporating the distillation token. Additionally,
CvT [32] and CeiT [36] integrate the convolutional struc￾ture into the ViT framework to combine the strengths of
CNNs (spatial invariance) and ViTs (long-range depen￾dency modeling). These advancements underscore the on￾going evolution of transformer-based architectures in the
field of computer vision.
2.2. Efficient Vision Transformers
Though highly effective, ViTs suffer from a huge com￾putational burden. The research on efficient vision trans￾formers addresses the quadratic cost of the self-attention
operation by including hierarchical downsampling opera￾tions [17, 29, 30], token reduction [9, 21, 33], or lightweight
architectural designs [18, 19]. Hierarchical downsam￾pling operations address the quadratic computation of self￾attention by reducing the token numbers gradually across
the stages [17, 18, 29, 30] and enable ViTs to learn hierar￾chical structures. Another research direction introduces the
token selection module to eliminate the least meaningful to￾kens and reduce the computational burden. For instance,
[9, 21, 33] reorganize image tokens by preserving informa- Patch Emb
Encoder
Patch Emb
Encoder
Patch Emb
Encoder
Patch Emb
Encoder
Patch Emb
Norm
MHSA
Norm
MLP
Linear
Transpose
Linear
Transpose
Softmax
MatMul
Figure 2. The downsampling on attention across stages.
tive image tokens and dropping ones with little attention to
expedite subsequent MHSA and FFN computations.
2.3. Attention Mechanisms
The key component of ViTs is the attention mechanism,
which computes pairwise interactions between all patches,
resulting in quadratic complexity with respect to the input
size. This problem leads to heavy inference computation,
which hinders the practical application of ViTs in the real
world. Several studies argue that the computational burden
can be alleviated by utilizing sparse attention mechanisms,
which selectively attend to a subset of patches based on their
relevance or proximity. One notable approach is the Adap￾tive Sparse Token Pruning framework [31], which induces
a sparse attention matrix, effectively addressing computa￾tional efficiency concerns. Furthermore, employing tech￾niques like structured sparsity patterns [3, 6, 21] can fur￾ther reduce computational complexity, thereby enhancing
the overall efficiency of ViTs. Another urgent issue to be
addressed is the problem of attention saturation, where the
attention matrix displays limited variation as the layer depth
increases. This issue has been acknowledged in studies such
as DeepViT [38] and CaiT [26], which report that attention
saturation hinders the ability of deep ViTs to capture addi￾tional semantic information and may even reduce training
stability. Therefore, it is essential to carefully design the
self-attention mechanism in ViTs to avoid sub-optimal so￾lutions.
3. Methodology
In this section, we first review the basic design of the hier￾archical vision transformer. Then, we discuss two major
weaknesses of its attention mechanism, and propose that
self-attention can be grasped with less attention at each
stage. We dynamically re-parameterize the attention scores
by utilizing the stored attention matrix from the previous
layer, effectively mitigating the issue of attention saturation.
Additionally, we integrate residual connections to facilitate
the transfer of global relationships from earlier stages. Last
but not least, we introduce a novel loss, the Diagonality Pre￾serving loss, to preserve basic properties in the transformed
attention (i.e., representing the relationships among tokens).
3.1. Vision Transformer
Let x ∈ R
H×W×C represent an input image, where
H × W denotes the spatial resolution and C the num￾ber of channels. We first tokenize the image by parti￾tioning it into N = HW/p
2 patches, where each patch
Pi ∈ R
p×p×C (i ∈ {1, . . . , N}) has a size of p × p pix￾els and C channels. The patch size p is a hyper-parameter
that determines the granularity of the token. The patch em￾bedding can be extracted by using a convolution operator
with the stride and kernel size equal to the patch size. Each
patch is then projected to the embedding space Z ∈ R
N×D
through non-overlapping convolution, where D represents
the dimension of each patch.
Multi-Head Self-Attention. We first provide a brief
overview on the vanilla self-attention mechanism that pro￾cesses the embedded patches and functions within the
framework of Multi-Head Self-Attention blocks (MHSAs).
In the l-th MHSA block, the input Zl−1, l ∈ {1, · · · , L},
is projected into three learnable embeddings {Q, K, V} ∈
R
N×D. The multi-head attention aims to capture the at￾tention from different views; for simplicity, we choose H
heads, where each head is a matrix with the dimension
N ×
D
H
. The h-th head attention matrix Ah can be cal￾culated by:
 \mathbf {A}_h = \mathrm {Softmax} \left (\frac {\mathbf {Q}_h \mathbf {K}_h^\mathsf {T}}{\sqrt {d}} \right ) \in \mathbb {R}^{N \times N}. \label {eq:attn} (1)
Ah, Qh, and Kh are the attention matrix, query, and key
of the h-th head, respectively. We also split the value V
into H heads. To avoid vanishing gradients caused by the
sharpness of the probability distribution, we divide the inner
product of Qh and Kh by √
d (d = D/H). The attention
matrix is concatenated as:
 \begin {split} \mathbf {A} &= \textrm {Concat}(\mathbf {A}_1, \cdots , \mathbf {A}_h, \cdots ,\mathbf {A}_H); \\ \mathbf {V} &= \textrm {Concat}(\mathbf {V}_1, \cdots , \mathbf {V}_h, \cdots ,\mathbf {V}_H). \end {split} \label {eq:concat} 
(2)
The calculated attention among spatially split tokens may
guide the model to focus on the most valuable tokens within
the visual data. Subsequently, weighted linear aggregation
is applied to the corresponding value V:
 \boldsymbol {Z}^{\textrm {MHSA}} = \mathbf {AV} \in \mathbb {R}^{N \times D}. \label {eq:val-feats} (3)
Downsampling Operation. Several studies have in￾corporated hierarchical structures into ViTs, drawing in￾spiration from the success of hierarchical architectures in
CNNs. These works partition the Transformer blocks into
M stages and apply downsampling operations before each
Transformer stage, thereby reducing the sequence length.
In our study, we employ a downsampling operation using
a convolutional layer with the kernel size and stride set to
2. This approach permits the flexible adjustment of the fea￾ture map’s scale at each stage, thereby establishing a Trans￾former hierarchical structure that mirrors the organization
of the human visual system.
3.2. The Less-Attention Framework
The overall framework of our network architecture is illus￾trated in Fig. 1. In each stage, we extract the feature rep￾resentation in two phases. At the initial several Vanilla At￾DWConv
Conv 
Layer 
Scale
tention (VA) layers, we conduct the standard MHSA opera￾tion to capture the overall long-range dependencies. Sub￾sequently, we simulate the attention matrices to mitigate
quadratic computation and address attention saturation at
the following Less-Attention (LA) layers by applying a lin￾ear transformation to the stored attention scores. Herein,
we denote the attention score before the Softmax function
of the initial l-th VA layer in the m-th stage as AVA,l
m , which
is computed by the following standard procedure:
 \mathbf {A}^{\text {VA},l}_m = \frac {\mathbf {Q}^l_m(\mathbf {K}^l_m)^\mathsf {T}}{\sqrt {d}}, ~~ l \leq L^{\text {VA}}_m. \label {eq:init} (4)
Here, Ql
m and Kl
m represent the queries and keys from
the l-th layer of the m-th stage, following the downsam￾pling from the preceding stage. And L
VA
m is used to denote
the number of VA layers. After the initial vanilla attention
phase, we discard the traditional quadratic MHSA and ap￾ply transformations on AVA
m to lessen the amount of atten￾tion computation. This process entails two linear transfor￾mations with a matrix transposition operation in between.
To illustrate, let us consider the attention matrix in the l-th
(l > LVA
m ) layer (LA layer) of the stage:
 \begin {aligned} &\mathbf {A}^{l}_m = \Psi (\Theta (\mathbf {A}^{l-1}_m)^\mathsf {T})^\mathsf {T}, ~~ L^{\text {VA}}_m<l \leq L_m,\\ &\mathbf {Z}^{\text {LA},l} = \textrm {Softmax}(\mathbf {A}^l_m)\mathbf {V}^l. \end {aligned} 
(5)
In this context, the transformations denoted by Ψ and Θ
refer to linear transformation layers with a dimension of
R
N×N . Here, Lm, L
VA
m represent the number of layers and
the number of VA layers in the m-th stage, respectively.
The insertion of the transposition operation between these
two linear layers serves the purpose of maintaining the ma￾trix similarity behavior. This step is essential due to the fact
that the linear transformation in a single layer conducts the
transformations row-wise, which could potentially result in
the loss of diagonal characteristics.
3.3. Residual-based Attention Downsampling
When the computation traverses across stages in hierarchi￾cal ViTs, downsampling operations are often employed on
the feature map. While this technique reduces the token
numbers, it may result in the loss of essential contextual in￾formation. Consequently, we posit that the attention affinity
learned from the preceding stage could prove advantageous
for the current stage in capturing more intricate global re￾lationships. Drawing inspiration from ResNet [7], which
introduces shortcut connections to mitigate feature satura￾tion issues, we adopt a similar concept and incorporate it
into the downsampling attention computation within our ar￾chitecture. By introducing a shortcut connection, we can
introduce the inherent bias into the current MHSA block.
This allows the attention matrix from the previous stage to
effectively guide the attention computation of the current
stage, thereby preserving crucial contextual information.
However, directly applying the shortcut connection to
the attention matrix might pose challenges in this context,
primarily due to the difference in the attention dimensions
between the current stage and the preceding stage. Here,
we design an Attention Residual (AR) module that con￾sists of a depth-wise convolution (DWConv) and a Conv1×1
layer to downsample the attention map from the previous
stage while keeping the semantic information. We denote
the last attention matrix (at the Lm−1 layer) of the previ￾ous stage (the m − 1-th stage) as A
last
m−1
, and the downsam￾pled initial attention matrix of the current (the m-th) stage
as A
init
m . A
last
m−1 has the dimension of R
B×H×Nm−1×Nm−1
(Nm−1 denotes the token number at the m − 1-th stage).
We view the multi-head dimension H as the channel dimen￾sion in regular image space, thus with the DWConv operator
(stride = 2, kernel size = 2), we may capture the spatial
dependencies among the tokens during attention downsam￾pling. The output matrix after the DWConv transformation
fits the size of the attention matrix of the current stage, i.e.,
R
B×H×Nm×Nm(Nm = Nm−1/2 in our case). After depth￾width convolution on the attention matrix, we then perform
Conv1×1 to exchange information across different heads.
Our attention downsampling is illustrated in Fig. 2, and the
transformation from A
last
m−1
to A
init
m can be expressed as:
 \textbf {A}^\textrm {init}_m &= \textrm {Conv}_{1\times 1}\left (\textrm {Norm}(\textrm {DWConv}(\textbf {A}^\textrm {last}_{m-1}))\right ), \label {eq:residual} \\ \mathbf {A}^{\text {VA}}_m &\gets \mathbf {A}^{\text {VA}}_m + \textrm {LS}(\textbf {A}^\textrm {init}_m) \label {eq:plus},
(7)
where LS is the Layer-Scale operator introduced in [26] to
alleviate attention saturation. AVA
m is the attention score
for the first layer in the m-th stage, which is calculated by
adding the standard MHSA with Eq. 4 and the residual cal￾culated by Eq. 6.
Two fundamental design principles guide our atten￾tion downsampling module. First, we utilize DWConv to
capture spatial local relationships during downsampling,
thereby enabling the efficient compression of attention re￾lationships. Second, the Conv1×1 operation is utilized to
exchange the attention information across heads. This de￾sign is pivotal as it facilitates the efficient propagation of
attention from the preceding stage to the subsequent stage.
Incorporating the residual attention mechanism necessitates
only minor adjustments, typically involving adding a few
lines of code to the existing ViT backbone. It is worth em￾phasizing that such a technique can be seamlessly applied to
various versions of the Transformer architecture. The only
prerequisite is to store the attention scores from the previ￾ous layer and establish the skip-connections to this layer
accordingly. The importance of this module will be further
illuminated through comprehensive ablation studies.
3.4. Diagonality Preserving Loss
We have carefully designed the Transformer modules by
incorporating attention transformation operators, aiming to
mitigate the issues of computational cost and attention sat￾uration. However, a pressing challenge remains—ensuring
that the transformed attention preserves the inter-token re￾lationships. It is well established that applying transforma￾tions to attention matrices can compromise their capacity to
capture similarities, largely due to the linear transformation
treating the attention matrix row-wise. Thus, we design a
an alternative approach to guarantee that the transformed at￾tention matrix retains the fundamental properties necessary
to convey associations among tokens. A conventional at￾tention matrix should possess the following two properties,
i.e., diagonality and symmetry:
 \begin {aligned}[b] \mathbf {A}_{ij} &= \mathbf {A}_{ji}, \\ \mathbf {A}_{ii} &> \mathbf {A}_{ij}, \forall j \neq i. \end {aligned} \label {eq:property} 
(8)
Thus, we design the diagonality preserving loss of the l-th
layer to keep these two basic properties as:
 \begin {split} {\mathcal {L}_{\textrm {DP},l}} &= \sum _{i=1}^N\sum _{j=1}^N\left |\mathbf {A}_{ij} -\mathbf {A}_{ji}\right | \\ &+ \sum _{i=1}^N((N-1)\mathbf {A}_{ii}-\sum _{j\neq i}\mathbf {A}_{j}). \end {split} 
(9)
Here, LDP is the Diagonality Preserving loss aiming at pre￾serving the properties of attention matrix of Eq. 8. We add
our Diagonality Preserving Loss on all transformation lay￾ers with the vanilla cross-entropy (CE) loss [4], thus the
total loss in our training can be presented as:
 \begin {aligned}[b] \mathcal {L}_\textrm {total} &= \mathcal {L}_\textrm {CE} + \sum _{m=1}^M\sum _{l=1}^{L_m}\mathcal {L}_{\textrm {DP},l}, \\ \mathcal {L}_\textrm {CE} &= \textrm {cross-entropy}(Z_\texttt {Cls}, y), \end {aligned} 
(10)
where ZCls is the classification token of the representation
in the last layer.
3.5. Complexity Analysis
Our architecture consists of four stages, each comprising
Lm layers. The downsampling layer is applied between
each consecutive stage. As such, the computational com￾plexity of traditional self-attention is O(N2
mD), whereas
the associated K-Q-V transformation incurs a complex￾ity of O(3NmD2
). In contrast, our method leverages an
Nm × Nm linear transformation within the transformation
layer, thereby circumventing the need for computing the in￾ner products. Consequently, the computation complexity
of our attention mechanism in the transformation layer is
Table 1. Detailed configurations of the LaViT series. ’Blocks’ and
’Heads’ refer to the number of blocks ([L
1
, L2
, L3
, L4
]) and heads
in four stages, respectively. ’Channels’ refers to the input channel
dimensions across the four stages. And ’NLA’ denotes the layer
within each stage at which the utilization of the Less-Attention
layer begins.
Models Channels Blocks Heads NLA
LaViT-T [64,128,320,512] [2,2,2,2] [1,2,5,8] [0,0,2,2]
LaViT-S [64,128,320,512] [3,4,6,3] [1,2,5,8] [0,0,3,2]
LaViT-B [64,128,320,512] [3,3,18,3] [1,2,5,8] [0,2,4,3]
reduced to O(N2
m), representing a reduction factor of D.
Additionally, since our method calculates the query em￾beddings solely within the Less-Attention layer, our K-Q-V
transformation complexity is likewise diminished by a fac￾tor of 3.
In the downsampling layer between consecutive stages,
considering a downsample rate of 2 as an example, the
computational complexity of the DWConv in the attention
downsampling layer can be calculated as Complexity =
2×2×
Nm
2 ×
Nm
2 ×D = O(N2
mD). Similarly, the complex￾ity of the Conv1×1 operation in the attention residual mod￾ule is also O(N2
mD). However, it is important to note that
attention downsampling only occurs once per stage. There￾fore, the additional complexity introduced by these opera￾tions is negligible when compared to the complexity reduc￾tion achieved by the Less-Attention layer.
4. Experiments
In this section, we evaluate our model’s performance on two
benchmark datasets: ImageNet-1K [8] for classification,
COCO2017 [10] for detection and ADE20K [37] for seg￾mentation. We compare our model with other state-of-the￾art works on these datasets to demonstrate its effectiveness
and efficiency. Furthermore, we perform ablation studies to
investigate the necessity and contributions of each compo￾nent in the proposed model. This analysis provides valuable
insights into the role of each part and helps to establish the
efficacy of our approach.
4.1. Architecture Variants
To ensure an equitable comparison with other models while
maintaining a similar level of computational complexity,
we establish three models: LaViT-T, LaViT-S, and LaViT￾B. The detailed configuration information is provided in
Table 1, and we follow the same network structure as
1The 90% notation in brackets indicates that we keep the token ratio of
90% to represent the visual data during the training of the corresponding
ViTs—DynamicViT and EviT, respectively. Additionally, given our aim
to strike a balance between efficiency and effectiveness, we will not com￾pare our results to high-performance but computationally intensive models,
such as Swin-B-V2 [14] and ConvNeXt-B [15].
Model Params FLOPs Throughput Top1
(M) (G) (image/s) (%)
ResNet-18 11.7 1.8 4454 69.8
RegNetY-1.6G 11.2 1.6 1845 78.0
DeiT-T 5.7 1.3 3398 72.2
PVT-T 13.2 1.9 1768 75.1
PVTv2-b1 13.1 2.1 1231 78.7
LaViT-T 10.9 1.6 2098 79.2
ResNet-50 25.0 4.1 1279 76.2
RegNetY-4G 20.6 4.0 1045 79.4
EfficientNet-B4 19.0 4.2 387 82.4
EfficientViT-B2 24.0 4.5 1587 82.1
DeiT-S 22.1 4.6 1551 79.9
DeepViT-S 27.0 6.2 1423 82.3
PVT-S 24.5 3.8 1007 79.8
CvT-S 25.8 7.1 636 82.0
Swin-T 28.3 4.5 961 81.2
PVTv2-b2 25.4 4.0 695 82.0
DynamicViT-S (90%) 24.1 4.0 1524 79.8
EViT-S (90%) 23.9 4.1 1706 79.7
LiT-S 27.0 4.1 1298 81.5
PPT-S 22.1 3.1 1698 79.8
LaViT-S 22.4 3.3 1546 82.6
ResNet-101 45.0 7.9 722 77.4
ViT-B 86.6 17.6 270 77.9
DeiT-B 86.6 17.5 582 81.8
Swin-S 49.6 8.7 582 83.1
Swin-B 87.8 15.4 386 83.4
DynamicViT-B (90%) 76.6 14.1 732 81.5
EViT-B (90%) 78.6 15.3 852 81.3
LiT-M 48.0 8.6 638 83.0
PPT-B 86.0 14.5 714 81.4
PVT-M 44.2 6.7 680 81.2
PVT-L 61.4 9.8 481 81.7
LaViT-B 39.6 6.1 877 83.1
Table 2. Comparison of different backbones on ImageNet-1K clas￾sification. Except for EfficientNet (EfficientNet-B4), all models
are trained and evaluated with an input size of 224 × 224. The
least computations and fastest throughput appear in blue bold, and
the best results appear in bold.
1
PVT [29, 30] except for introducing the Less-Attention
Transformer encoder and skip-connection attention down￾sampling. The number of blocks, channels, and heads af￾fects the computational cost.
4.2. Baselines
We conduct a thorough experimental evaluation of our pro￾posed method by comparing it with various CNNs, ViTs,
and hierarchical ViTs. Specifically, the following baselines
are used:
• CNNs: ResNet [7], RegNet [20] and EfficientNet [24].
• ViTs: ViT [4], DeiT [25], CvT [36],DeepViT [38], Fo￾calViT [35] and SwinTransformer [13].
• Efficient ViTs: HVT [17], PVT [29], DynamicViT [21],
EViT [9], LiT [19], EfficientViT [2] and PPT [33].
4.3. Image Classification on ImageNet-1K
Settings. The image classification experiments are con￾ducted on the ImageNet-1K dataset. Our experimental pro￾tocol follows the procedures outlined in DeiT [25], with the
exception of the model itself. Specifically, we apply the
same data augmentation and regularization techniques em￾ployed in DeiT. We utilize the AdamW optimizer [16] to
train our models from scratch for 300 epochs (with a 5-
epoch warm-up). The initial learning rate is set to 0.005 and
varies according to a cosine scheduler. The global batch size
is set to 1024, distributed across 4 GTX-3090 GPUs. Dur￾ing the test on the validation set, the input images are first
resized to 256 pixels, followed by a center crop of 224 x
224 pixels to evaluate the classification accuracy.
Results. We present the classification results on ImageNet-
1K in Table 2. The models are classified into three groups
based on their computational complexity: tiny (approxi￾mately 2G), small (approximately 4G), and base (approx￾imately 9G). Our approach achieves competitive perfor￾mance compared to state-of-the-art ViTs with markedly re￾duced computational requirements. Specifically, in the tiny
and small model clusters, our method surpasses all other ex￾isting models by at least 0.2% and 0.5%, respectively, while
maintaining a substantially lower computational cost, which
is our principal concern. In the base-size models, our archi￾tecture, which incorporates the base structure of PVT but
includes the Less-Attention component, demonstrates supe￾rior performance over two PVT-based models (PVT-M and
PVT-L). Furthermore, we also compare our architecture to
several efficient ViT designs (DynamicViT, EViT, LiT, effi￾cientViT and PPT). We observe that our results reflect a bet￾ter balance between effectiveness and efficiency. Note that
our design necessitates reduced computation cost owing to
our resource-efficient Less-Attention mechanism, rendering
our lightweight module an attractive option for implement￾ing ViT on mobile platforms.
4.4. Object Detection on COCO2017
Settings. We conduct the detection experiments on COCO
2017 [10] dataset. We test the model effectiveness on Reti￾naNet [12]. We follow the common practice by initializ￾ing the backbone with pre-trained weights obtained from
ImageNet-1K. In addition, we use AdamW [16] optimizer,
and train the network with the batchsize of 16 on 8 GPUs.
Results. We present the results of object detection in Ta￾ble 3. It is evident that our LaViT model exhibits a no￾table advantage over both its CNN and Transformer coun￾terparts. Specifically, with the 1× schedule, our tiny ver￾sion LaViT-T achieves 9.9-12.5 APb
against ResNet under
comparable settings, while the small version LaViT-S out￾performs its CNN counterpart by 8.1-10.3 APb
. This trend
persists with the 3× schedule, as our LaViT consistently
demonstrates competitive performance. Particularly note￾worthy is our architecture’s ability to consistently outper￾form the Swin Transformer in terms of detection perfor￾mance while imposing a smaller training burden. Thus,
the results on COCO2017 reaffirm our assertion that our
Backbone #Param. FLOPs RetinaNet 1× RetinaNet 3× + MS
(M) (G) APb APb
50 APb
75 APb
S APbM APb
L APb APb
50 APb
75 APb
S APbM APb
L
ResNet50 38 239 36.3 55.3 38.6 19.3 40.0 48.8 39.0 58.4 41.8 22.4 42.8 51.6
PVT-Small 34 226 40.4 61.3 43.0 25.0 42.9 55.7 42.2 62.7 45.0 26.2 45.2 57.2
Swin-T 39 245 41.5 62.1 44.2 25.1 44.9 55.5 43.9 64.8 47.1 28.4 47.2 57.8
LaViT-T(ours) 33 202 46.2 67.2 49.1 29.6 50.2 61.3 48.4 69.9 51.7 31.8 52.2 64.1
ResNet101 58 315 38.5 57.8 41.2 21.4 42.6 51.1 40.9 60.1 44.0 23.7 45.0 53.8
PVT-M 54 283 41.9 63.1 44.3 25.0 44.9 57.6 43.2 63.8 46.1 27.3 46.3 58.9
Swin-S 60 335 44.5 65.7 47.5 27.4 48.0 59.9 46.3 67.4 49.8 31.1 50.3 60.9
LaViT-S(ours) 47 290 46.7 68.3 49.7 29.9 50.7 61.7 48.9 70.3 52.2 33.1 52.6 65.4
Table 3. Results on COCO object detection using the RetinaNet [12] framework. 1× refers to 12 epochs, and 3× refers to 36 epochs. MS
means multi-scale training. APb
and APm denotes box mAP and mask mAP, respectively. FLOPs are measured at resolution 800 × 1280.
5 10 15 20 25
Layer Index
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
ViT
ViT+LA
(a) ViT
5 10 15
Layer Index
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
PVT
PVT+LA
(b) PVT
Figure 3. The similarity ratio of the generated self-attention maps
of the current layer with its previous layer.
carefully designed LaViT model enhances feature extrac￾tion with reduced computational overhead.
4.5. Semantic Segmentation on ADE20K
Settings. We conduct experiments on semantic segmen￾tation using the ADE20K dataset, which comprises 150
classes and 20,000 images for training, and 2,000 images
for validation. Our backbone networks for segmentation
are Semantic FPN [11] and UperNet [34]. We follow the
training settings established in [13] and resize images to
512 × 512 for training. We train UperNet for 160k itera￾tions and SemanticFPN for 80k iterations. The initial learn￾ing rate is set to 6 × 10−5
, utilizing a poly scheduler for
learning rate decay. The experiment is conducted by using
the batch size of 16 across 4 GTX3090 GPUs.
Results. Table 4 provides an overview of the segmenta￾tion results. Our model demonstrates superiority over Swin
Transformer, exhibiting an mIoU improvement of +2.6 with
Semantic FPN and +2.7 with UperNet. In the Semantic FPN
test, our LaViT-S achieves a relatively modest increase of
+0.9 mIoU compared to the baseline (PVT-S), but notably
with significantly fewer computations. When integrated
into the UperNet architecture, LaViT achieves substantial
improvements of +2.7 mIoU, +1.0 mIoU, and +1.4 mIoU
when compared to various mainstream models. These com￾petitive results are maintained even when employing test
time augmentation. In particular, LaViT-S outperforms
Focal-T by +1.4 mIoU and +2.5 MS mIOU. These findings
underscore LaViT’s ability to produce high-quality seman￾tic segmentation outputs while operating within the frame￾work of its computation-efficient attention mechanism.
4.6. Ablation Study
Attention Saturation. To demonstrate the efficacy of our
Less-Attention module in addressing attention saturation,
we present the attention similarity ratio (cosine similarity
computed by the attention map in the current layer and its
previous layer) in Figure 3. We conduct the comparison
using two backbones, namely, ViT and PVT. In 3a, we se￾lect the ViT architecture with 25 layers and no hierarchical
structure. In 3b, we employ PVT-M as our baseline and
assess the attention similarity at the 3rd stage, which con￾sists of 18 layers. Both sets of results clearly illustrate that
the original architecture encounters a significant attention
saturation issue. However, this phenomenon is effectively
mitigated by incorporating our modules, enabling deep
attention to fulfill its intended role.
Extendability of Less-Attention Module. We extend our
Less-Attention module to various ViT architectures, and
report the results in Table 5. The incorporation of the Less￾Attention layer into any of the foundational Transformer
architectures leads to enhancements in accuracy while
concurrently reducing computational demands. Notably,
the most significant improvement is observed when incor￾porating the module into the vanilla ViT/DeiT architecture.
This may be attributed to the fact that the vanilla ViT/DeiT
does not have a hierarchical structure, thereby experiencing
considerable attention saturation issues. Moreover, when
integrating our method into DeepViT, we observe the most
substantial decrease in computational resources. These
findings jointly underscore the scalability of our method,
demonstrating that the application of LA module can render
existing ViT architectures more practical and feasible.
Importance of Each Component. We conduct ablation
studies on the proposed module with the ImageNet-1k
dataset, and the results are shown in Table 6. On both
networks (i.e., tiny and small), our proposed modules prove
to be indispensible for Transformer training. The baseline,
which replaces the Less-Attention layer with MHSA, cor￾responds exactly to the PVT model, exhibiting a decrease
in predictive accuracy by 0.5% and 0.6% compared to
our model. Additionally, removing the attention residual
modules, denoted as “w/o AR”, results in a reduction of
predictive accuracy by 0.2% and 0.4%. Lastly, and most
Cosine Similarity
Backbone Semantic FPN 80k UperNet 160K
Param (M) FLOPs (G) mIOU (%) Param (M) FLOPs (G) mIOU (%) MS mIOU (%)
ResNet-50 28.5 183 36.7 - - - -
Swin-T 31.9 182 41.5 59.9 945 44.5 45.8
PVT-S 30.2 146 43.2 - - - -
Twin-S 28.3 144 43.2 54.4 932 46.2 47.1
LiT-S 32.0 172 41.3 57.8 978 44.6 45.9
Focal-T - - - 62.0 998 45.8 47.0
LaViT-S 25.1 122 44.1 52.0 920 47.2 49.5
Table 4. Segmentation performance of different backbones in Semantic FPN and UpperNet framework on ADE20K. The least computation
appears in blue bold, and the best results appear in bold.
Backbone Tiny Small
Top-1 Acc(%) FLOPs (G) Top-1 Acc(%) FLOPs (G)
ViT 72.2 1.4 79.1 4.6
ViT+LA 73.2(↑ 1.0) 1.2(↓ 14.2%) 80.0(↑ 0.9) 4.0(↓ 13.1%)
DeiT 72.2 1.4 79.9 4.7
DeiT+LA 73.4(↑ 1.2) 1.2(↓ 14.2%) 80.4(↑ 0.5) 4.2(↓ 10.6%)
DeepViT 73.4 1.5 80.9 4.8
DeepViT+LA 73.8(↑ 0.4) 1.1(↓ 25.8%) 81.4(↑ 0.5) 4.2(↓ 12.6%)
CeiT 76.2 1.2 82.0 4.5
CeiT+LA 76.7(↑ 0.5) 1.1(↓ 9.0%) 82.4(↑ 0.4) 4.1(↓ 8.8%)
HVT 75.7 1.4 80.4 4.6
HVT+LA 76.2(↑ 0.5) 1.2(↓ 15.2%) 80.8(↑ 0.4) 4.2(↓ 13.4%)
PVT 75.1 1.9 79.8 3.8
PVT+LA 75.9(↑ 0.8) 1.4(↓ 25.6%) 80.4(↑ 0.6) 3.2(↓ 15.7%)
Swin 81.2 4.5 83.2 8.7
Swin+LA 81.7(↑ 0.5) 4.0(↓ 11.1%) 83.5(↑ 0.3) 7.8(↓ 10.3%)
Table 5. Top-1 classification accuracy on ImageNet-1k using
different transformer backbones and their corresponding Less￾Attention plug-in variants. Footnote ’LA’ indicates the addition
of our Less-Attention module to the respective backbone archi￾tectures. ↑ and ↓ denote the increase in Top-1 accuracy and the
percentage of FLOPs reduction, respectively.
Model Module Tiny Small AR LA LDP
w/o LA - - - 78.7 82.0
w/o AR - ✓ ✓ 79.0 82.2
LaViT ✓ ✓ ✓ 79.2 82.6
w/o LDP ✓ ✓ - 59.1(↓ 20.1) 57.1(↓ 25.5)
Table 6. Ablation study of the proposed module on the ImageNet-
1k dataset. Baseline means we remove all the proposed modules,
resulting in the PVT Transformer baseline. ’AR’ and ’LA’ indicate
the Attention-Residual and Less-Attention modules, respectively.
’w/o LDP’ indicates we remove the Diagonality Preserving loss.
importantly, we assert that the additional loss function
to preserve diagonal correlations is vital for effectively
comprehending semantic information in visual data. When
relying solely on the CE loss, the model’s predictions
deteriorate. This might be attributed to the potential limi￾tation of relying solely on the transformation for attention
matrices, which could compromise their capacity to express
correlations among tokens. All these experimental findings
collectively emphasize the contribution of each component
within our model architecture.
Model Stage 3 Stage 4
L2 L3 L4 L1 L2 L3
LaViT-S 79.1 82.6 82.4 80.1 82.6 82.3
LaViT-B 78.9 82.5 83.1 80.4 82.3 83.1
Table 7. Ablation study on the layer where Less-Attention starts.
We conduct experiments on the last two stages—Stage 3,4. ’L2’
beneath Stage 3 means we use the Less-Attention layer to replace
the vanilla encoder from the second layer in the third Stage.
Less-Attention Selection. In deep ViTs, careful selection
of the starting layer for Less-Attention is crucial. Thus,
we design experiments to select the starting layer for Less￾Attention in the network architecture, and the results are
presented in Table 7. As shown in the table, directly us￾ing the Less-Attention layer from the second layer in the
stage leads to a decrease in model performance. This phe￾nomenon could be attributed to overly relying on the se￾mantics of the first MHSA layer. Thus, leveraging the Less￾Attention layer at deeper layers in the stage may mitigate
this issue. Furthermore, while utilizing the Less-Attention
layer at relatively deeper layers does not affect the model
performance much, it may lead to increased computational
costs. This contradicts the design objective of our architec￾ture to reduce the computational overhead.
5. Conclusion
Aiming to reduce the costly self-attention computations, we
proposed a new model called Less-Attention Vision Trans￾former (LaViT). LaViT leverages the computed dependency
in Multi-Head Self-Attention (MHSA) blocks and bypasses
the attention computation by re-using attentions from previ￾ous MSA blocks. We additionally incorporated a straight￾forward Diagonality Preserving loss, designed to promote
the intended behavior of the attention matrix in represent￾ing relationships among tokens. Notably, our Transformer
architecture effectively captures cross-token associations,
surpassing the performance of the baseline while maintain￾ing a computationally efficient profile in terms of quan￾tity of parameters and floating-point operations per second
(FLOPs). Comprehensive experimentation has confirmed
the efficacy of our model as a foundational architecture
for multiple downstream tasks. Specifically, the proposed
model demonstrates superiority over previous Transformer
architectures, resulting in state-of-the-art performance in
classification and segmentation tasks.
References
[1] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.
Segnet: A deep convolutional encoder-decoder architecture
for image segmentation. TPAMI, pages 2481–2495, 2017. 1
[2] Han Cai, Chuang Gan, and Song Han. Efficientvit: Enhanced
linear attention for high-resolution low-computation visual
recognition. In IEEE/CVF ICCV, 2023. 6
[3] Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang,
and Zhangyang Wang. Chasing sparsity in vision transform￾ers: An end-to-end exploration. NeurIPS, pages 19974–
19988, 2021. 3
[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl￾vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In ICLR, 2021. 1, 2, 5, 6
[5] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Region-based convolutional networks for accurate
object detection and segmentation. TPAMI, pages 142–158,
2015. 1
[6] Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng, Ji￾aying Liu, and Jingdong Wang. Demystifying local vision
transformer: Sparse connectivity, weight sharing, and dy￾namic weight. arXiv:2106.04263, 2021. 3
[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR,
pages 770–778, 2016. 4, 6
[8] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classification with deep convolutional neural net￾works. Commun.ACM, pages 84–90, 2017. 1, 5
[9] Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song,
Jue Wang, and Pengtao Xie. Not all patches are what you
need: Expediting vision transformers via token reorganiza￾tions. arXiv:2202.07800, 2022. 1, 2, 6
[10] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and
C. Lawrence Zitnick. Microsoft COCO: common objects in
context. In ECCV, pages 740–755, 2014. 5, 6
[11] Tsung-Yi Lin, Piotr Doll´ar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyramid
networks for object detection. In CVPR, pages 2117–2125,
2017. 7
[12] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Doll´ar. Focal loss for dense object detection. In Pro￾ceedings of the IEEE international conference on computer
vision, pages 2980–2988, 2017. 6, 7
[13] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
ICCV, pages 9992–10002, 2021. 6, 7
[14] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,
Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.
Swin transformer v2: Scaling up capacity and resolution. In
CVPR, pages 12009–12019, 2022. 5
[15] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht￾enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s. In CVPR, pages 11976–11986, 2022. 5
[16] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. In ICLR, 2019. 6
[17] Zizheng Pan, Bohan Zhuang, Jing Liu, Haoyu He, and Jian￾fei Cai. Scalable vision transformers with hierarchical pool￾ing. In ICCV, pages 377–386, 2021. 2, 6
[18] Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Fast vision
transformers with hilo attention. NeurIPS, pages 14541–
14554, 2022. 2
[19] Zizheng Pan, Bohan Zhuang, Haoyu He, Jing Liu, and Jian￾fei Cai. Less is more: Pay less attention in vision transform￾ers. In AAAI, pages 2035–2043, 2022. 2, 6
[20] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,
Kaiming He, and Piotr Doll´ar. Designing network design
spaces. In CVPR, pages 10428–10436, 2020. 6
[21] Yongming rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie
Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision
transformers with dynamic token sparsification. NeurIPS,
34:13937–13949, 2021. 1, 2, 3, 6
[22] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Unified, real-time object de￾tection. In CVPR, pages 779–788, 2016. 1
[23] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In MICCAI, pages 234–241, 2015. 1
[24] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model
scaling for convolutional neural networks. In ICML, pages
6105–6114, 2019. 6
[25] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herv´e J´egou. Training
data-efficient image transformers & distillation through at￾tention. In ICML, pages 10347–10357, 2021. 2, 6
[26] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,
Gabriel Synnaeve, and Herv´e J´egou. Going deeper with im￾age transformers. In ICCV, pages 32–42, 2021. 3, 4
[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko￾reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. NeurIPS, 30, 2017. 1,
2
[28] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng
Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang.
Residual attention network for image classification. In
CVPR, pages 3156–3164, 2017. 1
[29] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao
Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.
Pyramid vision transformer: A versatile backbone for dense
prediction without convolutions. In ICCV, pages 568–578,
2021. 2, 6
[30] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao
Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt
v2: Improved baselines with pyramid vision transformer.
Computational Visual Media, pages 415–424, 2022. 2, 6
[31] Cong Wei, Brendan Duke, Ruowei Jiang, Parham Aarabi,
Graham W Taylor, and Florian Shkurti. Sparsifiner: Learn￾ing sparse instance-dependent attention for efficient vision
transformers. In CVPR, pages 22680–22689, 2023. 3
[32] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,
Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing
convolutions to vision transformers. In ICCV, pages 22–31,
2021. 2
[33] Xinjian Wu, Fanhu Zeng, Xiudong Wang, Yunhe Wang, and
Xinghao Chen. Ppt: Token pruning and pooling for efficient
vision transformers. arXiv:2310.01812, 2023. 1, 2, 6
[34] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and
Jian Sun. Unified perceptual parsing for scene understand￾ing. In ECCV, pages 418–434, 2018. 7
[35] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang
Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self￾attention for local-global interactions in vision transformers.
arXiv:2107.00641, 2021. 6
[36] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Feng￾wei Yu, and Wei Wu. Incorporating convolution designs into
visual transformers. In ICCV, pages 579–588, 2021. 2, 6
[37] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi￾dler, Adela Barriuso, and Antonio Torralba. Semantic un￾derstanding of scenes through the ADE20K dataset. Int. J.
Comput. Vis., pages 302–321, 2019. 5
[38] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang,
Xiaochen Lian, Zihang Jiang, Qibin Hou, and Jiashi
Feng. Deepvit: Towards deeper vision transformer.
arXiv:2103.11886, 2021. 1, 3, 6